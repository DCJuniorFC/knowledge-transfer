{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's cooking\n",
    "\n",
    "Referências usadas aqui:\n",
    "\n",
    "1. [machine-learning-articles/how-to-create-a-basic-mlp-classifier-with-the-keras-sequential-api.md at main · christianversloot/machine-learning-articles](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-create-a-basic-mlp-classifier-with-the-keras-sequential-api.md)\n",
    "\n",
    "2. [sklearn.naive_bayes.MultinomialNB — scikit-learn 1.1.1 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "3. [1.9. Naive Bayes — scikit-learn 1.1.1 documentation](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)\n",
    "\n",
    "4. [sklearn.neural_network.MLPClassifier — scikit-learn 1.1.1 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "\n",
    "5. [sklearn.svm.SVC — scikit-learn 1.1.1 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, precisamos importar os dados para nossa área de trabalho, usando pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28285</th>\n",
       "      <td>17632</td>\n",
       "      <td>indian</td>\n",
       "      <td>[tomatoes, flour, garlic, chopped cilantro, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>17601</td>\n",
       "      <td>french</td>\n",
       "      <td>[frozen chopped spinach, shallots, salt, whole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28691</th>\n",
       "      <td>36409</td>\n",
       "      <td>mexican</td>\n",
       "      <td>[eggs, all-purpose flour, unsalted butter, wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23163</th>\n",
       "      <td>508</td>\n",
       "      <td>indian</td>\n",
       "      <td>[ground ginger, tumeric, sunflower oil, frozen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33850</th>\n",
       "      <td>27261</td>\n",
       "      <td>mexican</td>\n",
       "      <td>[stock, fresh cilantro, sea salt, tortilla chi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  cuisine                                        ingredients\n",
       "28285  17632   indian  [tomatoes, flour, garlic, chopped cilantro, fr...\n",
       "5338   17601   french  [frozen chopped spinach, shallots, salt, whole...\n",
       "28691  36409  mexican  [eggs, all-purpose flour, unsalted butter, wat...\n",
       "23163    508   indian  [ground ginger, tumeric, sunflower oil, frozen...\n",
       "33850  27261  mexican  [stock, fresh cilantro, sea salt, tortilla chi..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_json('./train.json')\n",
    "test_df = pd.read_json('./test.json')\n",
    "\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo sido importados os dados, notamos que deseja-se prever a *cuisine* dada a lista de *ingredients*. Assim, para treinar modelos matemáticos baseados em **números** com propagação de gradiente para aprendizado, precisaremos transformar estes dados textuais em numéricos de alguma forma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada em quantos ingredientes cada receita tem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='ingredients', ylabel='Count'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(train_df['ingredients'].str.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualmente, os dados aparentam possuir distribuição gaussiana de média próxima de 10, com um máximo próximo a 30 ingredientes e poucas receita possuem poucos ingredientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features from Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma maneira bastante empregada para tokenizar texto (mapear palavras ou grupos delas, sentenças ou até contextos para valores numéricos) com resultados frequentemente satisfatórios no contexto de NLP é o encoding com base em [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model). Isto é, suponha as frases:\n",
    "\n",
    "```\n",
    "\"Eu sou Alexandre e gosto de IA\"\n",
    "\n",
    "\"Alexandre estuda IA\"\n",
    "```\n",
    "\n",
    "Elas podem ser mapeadas para:\n",
    "\n",
    "```\n",
    "[1 2 3 4 5 6]\n",
    "\n",
    "[3 8 7]\n",
    "```\n",
    "\n",
    "de maneira que:\n",
    "\n",
    "    1. = Eu\n",
    "    2. = sou\n",
    "    3. = Alexandre\n",
    "    4. = e\n",
    "    5. = gosto\n",
    "    6. = de\n",
    "    7. = IA\n",
    "    8. = estuda\n",
    "\n",
    "Este é um método de tokenizar frases. Outra maneira (que usaremos abaixo) é transformar cada frase em uma tabela com entradas binárias. Isto é, mapearemos as frases anteriores para uma tabela como esta:\n",
    "\n",
    "```\n",
    "    Eu  sou Alexandre   e   gosto   de  IA  estuda\n",
    "1   1   1   1           1   1       1   1   0\n",
    "2   0   0   1           0   0       0   1   1\n",
    "\n",
    "```\n",
    "\n",
    "Note que esse método faz com que percamos o sentido linear e semântico da sentença, mas isso não será um problema já que estamos lidando com uma sequência de ingredientes que podem ter sua ordem alterada (ou será que não?).\n",
    "\n",
    "Vamos unir os ingredientes (atualmente palavras listadas em um arranjo) em uma única frase para cada receita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        romaine lettuce black olives grape tomatoes ga...\n",
       "1        plain flour ground pepper salt tomatoes ground...\n",
       "2        eggs pepper salt mayonaise cooking oil green c...\n",
       "3                           water vegetable oil wheat salt\n",
       "4        black pepper shallots cornflour cayenne pepper...\n",
       "                               ...                        \n",
       "39769    light brown sugar granulated sugar butter warm...\n",
       "39770    KRAFT Zesty Italian Dressing purple onion broc...\n",
       "39771    eggs citrus fruit raisins sourdough starter fl...\n",
       "39772    boneless chicken skinless thigh minced garlic ...\n",
       "39773    green chile jalapeno chilies onions ground bla...\n",
       "Name: ingredients, Length: 39774, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coletando ingredientes:\n",
    "ingredients = train_df.ingredients.str.join(' ')\n",
    "ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos as frases que descrevem cada receita, e aplicaremos a elas uma remoção de palavras similares ou idênticas (extraíndo as raízes das palavras conjudadas), assim como pontuações e símbolos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10259</td>\n",
       "      <td>greek</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>romaine lettuce black olives grape tomatoes ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25693</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>plain flour ground pepper salt tomatoes ground...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20130</td>\n",
       "      <td>filipino</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>eggs pepper salt mayonaise cooking oil green c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22213</td>\n",
       "      <td>indian</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>water vegetable oil wheat salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13162</td>\n",
       "      <td>indian</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>black pepper shallots cornflour cayenne pepper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      cuisine                                        ingredients  \\\n",
       "0  10259        greek  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  25693  southern_us  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "2  20130     filipino  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
       "3  22213       indian                [water, vegetable oil, wheat, salt]   \n",
       "4  13162       indian  [black pepper, shallots, cornflour, cayenne pe...   \n",
       "\n",
       "                                                   X  \n",
       "0  romaine lettuce black olives grape tomatoes ga...  \n",
       "1  plain flour ground pepper salt tomatoes ground...  \n",
       "2  eggs pepper salt mayonaise cooking oil green c...  \n",
       "3                     water vegetable oil wheat salt  \n",
       "4  black pepper shallots cornflour cayenne pepper...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removendo caracteres especiais, stemming e lemmatizing:\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [stemmer.stem(lemmatizer.lemmatize(w)) for w in word_tokenize(text)]\n",
    "\n",
    "train = train_df.copy()\n",
    "train['X'] = train_df.ingredients.str.join(' ')\n",
    "train['X'].apply(lemmatize_text)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, transformaremos estas frases em uma matriz binária de palavras conforme descrito anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 features: ['00', '10', '100', '14', '15', '25', '33', '40', '43', '95', '96', 'abalone', 'abbamele', 'absinthe', 'abura', 'acai', 'accent', 'accompaniment', 'achiote', 'acid']\n",
      "Total features: 3010\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>25</th>\n",
       "      <th>33</th>\n",
       "      <th>40</th>\n",
       "      <th>43</th>\n",
       "      <th>95</th>\n",
       "      <th>...</th>\n",
       "      <th>zatarain</th>\n",
       "      <th>zatarains</th>\n",
       "      <th>zero</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "      <th>zinfandel</th>\n",
       "      <th>ziti</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>épices</th>\n",
       "      <th>num_ingr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3011 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  10  100  14  15  25  33  40  43  95  ...  zatarain  zatarains  zero  \\\n",
       "0   0   0    0   0   0   0   0   0   0   0  ...         0          0     0   \n",
       "1   0   0    0   0   0   0   0   0   0   0  ...         0          0     0   \n",
       "2   0   0    0   0   0   0   0   0   0   0  ...         0          0     0   \n",
       "3   0   0    0   0   0   0   0   0   0   0  ...         0          0     0   \n",
       "4   0   0    0   0   0   0   0   0   0   0  ...         0          0     0   \n",
       "\n",
       "   zest  zesty  zinfandel  ziti  zucchini  épices  num_ingr  \n",
       "0     0      0          0     0         0       0        16  \n",
       "1     0      0          0     0         0       0        19  \n",
       "2     0      0          0     0         0       0        20  \n",
       "3     0      0          0     0         0       0         5  \n",
       "4     0      0          0     0         0       0        33  \n",
       "\n",
       "[5 rows x 3011 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "vectorized_ingredients = count_vectorizer.fit_transform(train.X)\n",
    "print('20 features: '+ str(count_vectorizer.get_feature_names()[0:20]))\n",
    "print('Total features: '+ str(len(count_vectorizer.get_feature_names())))\n",
    "print(vectorized_ingredients.toarray()[0:10])\n",
    "\n",
    "vectorized_ingredients_df = pd.DataFrame(columns = count_vectorizer.get_feature_names(), data = vectorized_ingredients.toarray())\n",
    "vectorized_ingredients_df['num_ingr'] = vectorized_ingredients_df.sum(axis = 1)\n",
    "vectorized_ingredients_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos algumas estatísticas sobre estas palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>25</th>\n",
       "      <th>33</th>\n",
       "      <th>40</th>\n",
       "      <th>43</th>\n",
       "      <th>95</th>\n",
       "      <th>...</th>\n",
       "      <th>zatarain</th>\n",
       "      <th>zatarains</th>\n",
       "      <th>zero</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "      <th>zinfandel</th>\n",
       "      <th>ziti</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>épices</th>\n",
       "      <th>num_ingr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "      <td>39774.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.023407</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>20.573214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.005014</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.012281</td>\n",
       "      <td>0.017367</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.153975</td>\n",
       "      <td>0.024041</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>0.030895</td>\n",
       "      <td>0.151859</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>8.982429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>141.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 3011 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 00            10           100            14            15  \\\n",
       "count  39774.000000  39774.000000  39774.000000  39774.000000  39774.000000   \n",
       "mean       0.000025      0.000126      0.000025      0.000126      0.000075   \n",
       "std        0.005014      0.011211      0.005014      0.011211      0.008685   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 25            33            40            43            95  \\\n",
       "count  39774.000000  39774.000000  39774.000000  39774.000000  39774.000000   \n",
       "mean       0.000075      0.000151      0.000302      0.000025      0.000101   \n",
       "std        0.008685      0.012281      0.017367      0.005014      0.010028   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...      zatarain     zatarains          zero          zest  \\\n",
       "count  ...  39774.000000  39774.000000  39774.000000  39774.000000   \n",
       "mean   ...      0.000025      0.000025      0.000050      0.022452   \n",
       "std    ...      0.005014      0.005014      0.007091      0.153975   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      1.000000      1.000000      1.000000      3.000000   \n",
       "\n",
       "              zesty     zinfandel          ziti      zucchini        épices  \\\n",
       "count  39774.000000  39774.000000  39774.000000  39774.000000  39774.000000   \n",
       "mean       0.000578      0.000277      0.000955      0.023407      0.000101   \n",
       "std        0.024041      0.016628      0.030895      0.151859      0.010028   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      2.000000      1.000000   \n",
       "\n",
       "           num_ingr  \n",
       "count  39774.000000  \n",
       "mean      20.573214  \n",
       "std        8.982429  \n",
       "min        1.000000  \n",
       "25%       14.000000  \n",
       "50%       20.000000  \n",
       "75%       26.000000  \n",
       "max      141.000000  \n",
       "\n",
       "[8 rows x 3011 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_ingredients_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme visto no gráfico anterior e apresentado na coluna 'sum', muitas receitas (percentil de 75%) possuem 26 ou menos ingredientes. Similarmente, no percentil de 25% notamos de menos de 1/4 das receitas tem nuúmero de ingredientes igual ou menor a 14. \n",
    "\n",
    "Vejamos quanto cada palavra aparece em média:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00            0.000025\n",
       "10            0.000126\n",
       "100           0.000025\n",
       "14            0.000126\n",
       "15            0.000075\n",
       "               ...    \n",
       "zinfandel     0.000277\n",
       "ziti          0.000955\n",
       "zucchini      0.023407\n",
       "épices        0.000101\n",
       "num_ingr     20.573214\n",
       "Length: 3011, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_ingredients_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos então que algumas palavras (ingredientes) aparecem com uma frequência maior do que outras, ou seja, aparecem mais vezes em diferentes receitas. Por exemplo zucchini aparecem em média muito mais do que épices.\n",
    "\n",
    "Vamos então selecionar apenas as palavras que mais se relacionam com a saída. Para isso, primeiramente precisamos transformar as *cuisines* em números que representam a ordem em que cada cozinha aparece no banco de dados, representando numéricamente esta lista de palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 16,  4, ...,  8,  3, 13])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode (label) targets:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "cuisine = label_encoder.fit_transform(train_df['cuisine'].values)\n",
    "cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, finalmente, escolheremos as 1800 palavras mais relacionadas com as cozinhas a serem previstas, empregando a SelectKBest da biblioteca sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 1800)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "y = cuisine.copy()\n",
    "X = vectorized_ingredients_df.values\n",
    "num_selected = 1800\n",
    "X_count_selected = SelectKBest(chi2, k=num_selected).fit_transform(X, y)\n",
    "X_count_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs.: existem métodos mais sofisticados (no entanto eventualmente mais lentos) que podem resultar em performances melhores ou piores. Aqui utilizou-se a distribuição $\\chi^2$ (chi2) para distinguir entre palavras que nos informam mais ou menos sobre a saída."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features from Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discutimos anteriormente sobre como algumas palavras aparecem mais que outras. Uma forma de representar isso numericamente é através da vetorização TF-IDF (Term frequency * inverse document frequency). \n",
    "\n",
    "Este método consiste em calcular com qual frequência cada termo (ingrediente) aparece nos dados (TF). Em seguida, calcula-se em quantos documentos (receitas) estes termos aparecem (DF) e inverte-se algebricamente este valor ($\\frac{1}{DF}$ = IDF).\n",
    "\n",
    "Finalmente, multiplicando TF por IDF, obtemos TF-IDF, uma medida de quanto cada termo aparece no dados, normalizada pelo número de vezes em que esse termo apareceu nos registros que temos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 features: ['00', '10', '100', '14', '15', '25', '33', '40', '43', '95', '96', 'abalone', 'abbamele', 'absinthe', 'abura', 'acai', 'accent', 'accompaniment', 'achiote', 'acid']\n",
      "Total features: 3010\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>25</th>\n",
       "      <th>33</th>\n",
       "      <th>40</th>\n",
       "      <th>43</th>\n",
       "      <th>95</th>\n",
       "      <th>...</th>\n",
       "      <th>za</th>\n",
       "      <th>zatarain</th>\n",
       "      <th>zatarains</th>\n",
       "      <th>zero</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "      <th>zinfandel</th>\n",
       "      <th>ziti</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>épices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3010 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00   10  100   14   15   25   33   40   43   95  ...   za  zatarain  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0       0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0       0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0       0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0       0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0       0.0   \n",
       "\n",
       "   zatarains  zero  zest  zesty  zinfandel  ziti  zucchini  épices  \n",
       "0        0.0   0.0   0.0    0.0        0.0   0.0       0.0     0.0  \n",
       "1        0.0   0.0   0.0    0.0        0.0   0.0       0.0     0.0  \n",
       "2        0.0   0.0   0.0    0.0        0.0   0.0       0.0     0.0  \n",
       "3        0.0   0.0   0.0    0.0        0.0   0.0       0.0     0.0  \n",
       "4        0.0   0.0   0.0    0.0        0.0   0.0       0.0     0.0  \n",
       "\n",
       "[5 rows x 3010 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_vectorized_ingredients = tfidf_vectorizer.fit_transform(train.X)\n",
    "print('20 features: '+ str(tfidf_vectorizer.get_feature_names()[0:20]))\n",
    "print('Total features: '+ str(len(tfidf_vectorizer.get_feature_names())))\n",
    "print(tfidf_vectorized_ingredients.toarray()[0:10])\n",
    "\n",
    "tfidf_vectorized_ingredients_df = pd.DataFrame(columns = tfidf_vectorizer.get_feature_names(), data = tfidf_vectorized_ingredients.toarray())\n",
    "tfidf_vectorized_ingredients_df['sum'] = tfidf_vectorized_ingredients_df.sum(axis = 1)\n",
    "tfidf_vectorized_ingredients_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 1800)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = cuisine.copy()\n",
    "X = tfidf_vectorized_ingredients_df.values.copy()\n",
    "X_tfidf_selected = SelectKBest(chi2, k=num_selected).fit_transform(X, y)\n",
    "X_tfidf_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39774, 1800)\n",
      "(39774, 1800)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(39774, 3600)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join data\n",
    "y = cuisine.copy()\n",
    "print(X_count_selected.shape)\n",
    "print(X_tfidf_selected.shape)\n",
    "X = np.concatenate((X_count_selected.copy(),X_tfidf_selected.copy()), axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26648, 3600)\n",
      "(26648,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\tvalid_0's multi_logloss: 1.04829\n",
      "[30]\tvalid_0's multi_logloss: 0.873528\n",
      "[45]\tvalid_0's multi_logloss: 0.821136\n",
      "[60]\tvalid_0's multi_logloss: 0.803141\n",
      "[75]\tvalid_0's multi_logloss: 0.799371\n",
      "[90]\tvalid_0's multi_logloss: 0.799319\n",
      "Accuracy: 0.7754075879932958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       greek       0.85      0.49      0.62       164\n",
      " southern_us       0.61      0.41      0.49       271\n",
      "    filipino       0.78      0.67      0.72       530\n",
      "      indian       0.76      0.85      0.81       835\n",
      "    jamaican       0.74      0.48      0.58       225\n",
      "     spanish       0.59      0.60      0.59       911\n",
      "     italian       0.78      0.66      0.71       382\n",
      "     mexican       0.86      0.90      0.88       992\n",
      "     chinese       0.68      0.46      0.55       222\n",
      "     british       0.78      0.91      0.84      2581\n",
      "        thai       0.94      0.60      0.73       150\n",
      "  vietnamese       0.83      0.69      0.75       488\n",
      "cajun_creole       0.85      0.66      0.74       275\n",
      "   brazilian       0.89      0.92      0.91      2138\n",
      "      french       0.83      0.67      0.74       261\n",
      "    japanese       0.61      0.41      0.49       155\n",
      "       irish       0.68      0.81      0.74      1426\n",
      "      korean       0.67      0.48      0.56       320\n",
      "    moroccan       0.81      0.77      0.79       532\n",
      "     russian       0.72      0.54      0.62       268\n",
      "\n",
      "    accuracy                           0.78     13126\n",
      "   macro avg       0.76      0.65      0.69     13126\n",
      "weighted avg       0.78      0.78      0.77     13126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "X_train2, X_eval, y_train2, y_eval = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "lgbmC = lgb.LGBMClassifier()    \n",
    "lgbmC.fit(X_train2, y_train2,verbose=15,eval_set=(X_eval,y_eval))\n",
    "y_pred = lgbmC.predict(X_test)\n",
    "print('Accuracy: '+str(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=train.cuisine.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# explainer = shap.TreeExplainer(lgbmC)\n",
    "# shap_values = explainer.shap_values(X)\n",
    "# num of predicted classes\n",
    "# print(len(shap_values))\n",
    "# shap values for 0th class for 0th row\n",
    "# print(shap_values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7273350601858906\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       greek       0.56      0.38      0.45       164\n",
      " southern_us       0.46      0.45      0.46       271\n",
      "    filipino       0.57      0.75      0.65       530\n",
      "      indian       0.71      0.87      0.78       835\n",
      "    jamaican       0.70      0.44      0.54       225\n",
      "     spanish       0.52      0.55      0.53       911\n",
      "     italian       0.65      0.65      0.65       382\n",
      "     mexican       0.84      0.86      0.85       992\n",
      "     chinese       0.60      0.36      0.45       222\n",
      "     british       0.84      0.79      0.82      2581\n",
      "        thai       0.76      0.55      0.64       150\n",
      "  vietnamese       0.87      0.60      0.71       488\n",
      "cajun_creole       0.78      0.65      0.71       275\n",
      "   brazilian       0.91      0.87      0.89      2138\n",
      "      french       0.72      0.75      0.74       261\n",
      "    japanese       0.55      0.36      0.44       155\n",
      "       irish       0.58      0.71      0.64      1426\n",
      "      korean       0.50      0.52      0.51       320\n",
      "    moroccan       0.72      0.80      0.76       532\n",
      "     russian       0.67      0.49      0.57       268\n",
      "\n",
      "    accuracy                           0.73     13126\n",
      "   macro avg       0.68      0.62      0.64     13126\n",
      "weighted avg       0.74      0.73      0.73     13126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB(\n",
    "    alpha = 1.0\n",
    ")\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "print('Accuracy: '+str(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=train.cuisine.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.86789207\n",
      "Validation score: 0.326454\n",
      "Iteration 2, loss = 2.48627808\n",
      "Validation score: 0.397373\n",
      "Iteration 3, loss = 2.24179429\n",
      "Validation score: 0.487054\n",
      "Iteration 4, loss = 2.02331284\n",
      "Validation score: 0.538837\n",
      "Iteration 5, loss = 1.82529459\n",
      "Validation score: 0.577111\n",
      "Iteration 6, loss = 1.64850713\n",
      "Validation score: 0.606379\n",
      "Iteration 7, loss = 1.49926813\n",
      "Validation score: 0.636398\n",
      "Iteration 8, loss = 1.37253205\n",
      "Validation score: 0.665666\n",
      "Iteration 9, loss = 1.26510933\n",
      "Validation score: 0.690056\n",
      "Iteration 10, loss = 1.17614633\n",
      "Validation score: 0.702814\n",
      "Iteration 11, loss = 1.10133928\n",
      "Validation score: 0.716323\n",
      "Iteration 12, loss = 1.04117884\n",
      "Validation score: 0.723827\n",
      "Iteration 13, loss = 0.98957992\n",
      "Validation score: 0.729456\n",
      "Iteration 14, loss = 0.94907938\n",
      "Validation score: 0.739212\n",
      "Iteration 15, loss = 0.91080115\n",
      "Validation score: 0.747092\n",
      "Iteration 16, loss = 0.87652594\n",
      "Validation score: 0.741839\n",
      "Iteration 17, loss = 0.84628517\n",
      "Validation score: 0.749343\n",
      "Iteration 18, loss = 0.81955897\n",
      "Validation score: 0.758724\n",
      "Iteration 19, loss = 0.79863159\n",
      "Validation score: 0.759850\n",
      "Iteration 20, loss = 0.77631757\n",
      "Validation score: 0.760225\n",
      "Iteration 21, loss = 0.75887298\n",
      "Validation score: 0.762852\n",
      "Iteration 22, loss = 0.74371676\n",
      "Validation score: 0.766604\n",
      "Iteration 23, loss = 0.72623506\n",
      "Validation score: 0.767730\n",
      "Iteration 24, loss = 0.71240025\n",
      "Validation score: 0.768105\n",
      "Iteration 25, loss = 0.69948701\n",
      "Validation score: 0.769606\n",
      "Iteration 26, loss = 0.68663574\n",
      "Validation score: 0.771482\n",
      "Iteration 27, loss = 0.67513503\n",
      "Validation score: 0.772983\n",
      "Iteration 28, loss = 0.66605345\n",
      "Validation score: 0.773734\n",
      "Iteration 29, loss = 0.65606202\n",
      "Validation score: 0.773734\n",
      "Iteration 30, loss = 0.64746067\n",
      "Validation score: 0.774109\n",
      "Iteration 31, loss = 0.63976654\n",
      "Validation score: 0.777861\n",
      "Iteration 32, loss = 0.62896446\n",
      "Validation score: 0.776735\n",
      "Iteration 33, loss = 0.62310298\n",
      "Validation score: 0.780113\n",
      "Iteration 34, loss = 0.61452644\n",
      "Validation score: 0.778987\n",
      "Iteration 35, loss = 0.60756330\n",
      "Validation score: 0.778236\n",
      "Iteration 36, loss = 0.60241940\n",
      "Validation score: 0.780488\n",
      "Iteration 37, loss = 0.59445795\n",
      "Validation score: 0.780863\n",
      "Iteration 38, loss = 0.58957406\n",
      "Validation score: 0.781989\n",
      "Iteration 39, loss = 0.58559279\n",
      "Validation score: 0.781989\n",
      "Iteration 40, loss = 0.57813856\n",
      "Validation score: 0.782739\n",
      "Iteration 41, loss = 0.57108424\n",
      "Validation score: 0.783114\n",
      "Iteration 42, loss = 0.56696876\n",
      "Validation score: 0.780863\n",
      "Iteration 43, loss = 0.56220299\n",
      "Validation score: 0.780863\n",
      "Iteration 44, loss = 0.55732866\n",
      "Validation score: 0.785741\n",
      "Iteration 45, loss = 0.55085229\n",
      "Validation score: 0.777861\n",
      "Iteration 46, loss = 0.55003159\n",
      "Validation score: 0.782739\n",
      "Iteration 47, loss = 0.54423026\n",
      "Validation score: 0.786116\n",
      "Iteration 48, loss = 0.53877814\n",
      "Validation score: 0.786116\n",
      "Iteration 49, loss = 0.53564275\n",
      "Validation score: 0.782364\n",
      "Iteration 50, loss = 0.53142517\n",
      "Validation score: 0.782364\n",
      "Iteration 51, loss = 0.52686228\n",
      "Validation score: 0.788743\n",
      "Iteration 52, loss = 0.52416325\n",
      "Validation score: 0.786867\n",
      "Iteration 53, loss = 0.52163800\n",
      "Validation score: 0.785741\n",
      "Iteration 54, loss = 0.51709989\n",
      "Validation score: 0.787242\n",
      "Iteration 55, loss = 0.51347598\n",
      "Validation score: 0.784240\n",
      "Iteration 56, loss = 0.51333782\n",
      "Validation score: 0.783490\n",
      "Iteration 57, loss = 0.50840566\n",
      "Validation score: 0.786867\n",
      "Iteration 58, loss = 0.50374682\n",
      "Validation score: 0.788743\n",
      "Iteration 59, loss = 0.50047377\n",
      "Validation score: 0.789493\n",
      "Iteration 60, loss = 0.49663796\n",
      "Validation score: 0.787242\n",
      "Iteration 61, loss = 0.49539041\n",
      "Validation score: 0.788743\n",
      "Iteration 62, loss = 0.49205312\n",
      "Validation score: 0.789869\n",
      "Iteration 63, loss = 0.48900776\n",
      "Validation score: 0.790619\n",
      "Iteration 64, loss = 0.48765332\n",
      "Validation score: 0.787992\n",
      "Iteration 65, loss = 0.48418941\n",
      "Validation score: 0.790994\n",
      "Iteration 66, loss = 0.48199887\n",
      "Validation score: 0.791745\n",
      "Iteration 67, loss = 0.47916879\n",
      "Validation score: 0.793621\n",
      "Iteration 68, loss = 0.47647634\n",
      "Validation score: 0.790244\n",
      "Iteration 69, loss = 0.47288064\n",
      "Validation score: 0.787242\n",
      "Iteration 70, loss = 0.47093659\n",
      "Validation score: 0.790994\n",
      "Iteration 71, loss = 0.46885637\n",
      "Validation score: 0.788743\n",
      "Iteration 72, loss = 0.46650981\n",
      "Validation score: 0.787242\n",
      "Iteration 73, loss = 0.46374819\n",
      "Validation score: 0.787242\n",
      "Iteration 74, loss = 0.46059068\n",
      "Validation score: 0.787992\n",
      "Iteration 75, loss = 0.45840538\n",
      "Validation score: 0.789118\n",
      "Iteration 76, loss = 0.45710340\n",
      "Validation score: 0.790619\n",
      "Iteration 77, loss = 0.45437788\n",
      "Validation score: 0.789493\n",
      "Iteration 78, loss = 0.45487776\n",
      "Validation score: 0.790619\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.7834831631875666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       greek       0.78      0.55      0.65       164\n",
      " southern_us       0.61      0.54      0.57       271\n",
      "    filipino       0.78      0.62      0.69       530\n",
      "      indian       0.80      0.82      0.81       835\n",
      "    jamaican       0.67      0.59      0.63       225\n",
      "     spanish       0.64      0.62      0.63       911\n",
      "     italian       0.78      0.71      0.74       382\n",
      "     mexican       0.86      0.90      0.88       992\n",
      "     chinese       0.65      0.49      0.56       222\n",
      "     british       0.82      0.90      0.86      2581\n",
      "        thai       0.77      0.69      0.73       150\n",
      "  vietnamese       0.81      0.72      0.76       488\n",
      "cajun_creole       0.78      0.74      0.76       275\n",
      "   brazilian       0.91      0.92      0.91      2138\n",
      "      french       0.84      0.76      0.80       261\n",
      "    japanese       0.55      0.46      0.50       155\n",
      "       irish       0.70      0.80      0.75      1426\n",
      "      korean       0.54      0.54      0.54       320\n",
      "    moroccan       0.80      0.74      0.77       532\n",
      "     russian       0.63      0.61      0.62       268\n",
      "\n",
      "    accuracy                           0.78     13126\n",
      "   macro avg       0.74      0.69      0.71     13126\n",
      "weighted avg       0.78      0.78      0.78     13126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpc = MLPClassifier(random_state=1, \n",
    "    max_iter=150, #max_iter =300\n",
    "    early_stopping=True,\n",
    "    batch_size=int(np.round(X.shape[0]/10)),\n",
    "    verbose=True,    \n",
    "    learning_rate='adaptive'\n",
    "    ).fit(X_train, y_train)\n",
    "y_pred = mlpc.predict(X_test)\n",
    "print('Accuracy: '+str(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=train.cuisine.unique()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7834831631875666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       greek       0.78      0.55      0.65       164\n",
      " southern_us       0.61      0.54      0.57       271\n",
      "    filipino       0.78      0.62      0.69       530\n",
      "      indian       0.80      0.82      0.81       835\n",
      "    jamaican       0.67      0.59      0.63       225\n",
      "     spanish       0.64      0.62      0.63       911\n",
      "     italian       0.78      0.71      0.74       382\n",
      "     mexican       0.86      0.90      0.88       992\n",
      "     chinese       0.65      0.49      0.56       222\n",
      "     british       0.82      0.90      0.86      2581\n",
      "        thai       0.77      0.69      0.73       150\n",
      "  vietnamese       0.81      0.72      0.76       488\n",
      "cajun_creole       0.78      0.74      0.76       275\n",
      "   brazilian       0.91      0.92      0.91      2138\n",
      "      french       0.84      0.76      0.80       261\n",
      "    japanese       0.55      0.46      0.50       155\n",
      "       irish       0.70      0.80      0.75      1426\n",
      "      korean       0.54      0.54      0.54       320\n",
      "    moroccan       0.80      0.74      0.77       532\n",
      "     russian       0.63      0.61      0.62       268\n",
      "\n",
      "    accuracy                           0.78     13126\n",
      "   macro avg       0.74      0.69      0.71     13126\n",
      "weighted avg       0.78      0.78      0.78     13126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlpc.predict(X_test)\n",
    "print('Accuracy: '+str(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=train.cuisine.unique()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.87647202\n",
      "Validation score: 0.186492\n",
      "Iteration 2, loss = 2.62049840\n",
      "Validation score: 0.298687\n",
      "Iteration 3, loss = 2.51174493\n",
      "Validation score: 0.294934\n",
      "Iteration 4, loss = 2.45510331\n",
      "Validation score: 0.300188\n",
      "Iteration 5, loss = 2.39843815\n",
      "Validation score: 0.350094\n",
      "Iteration 6, loss = 2.34101580\n",
      "Validation score: 0.390994\n",
      "Iteration 7, loss = 2.28118795\n",
      "Validation score: 0.400375\n",
      "Iteration 8, loss = 2.21865367\n",
      "Validation score: 0.433021\n",
      "Iteration 9, loss = 2.15541584\n",
      "Validation score: 0.464916\n",
      "Iteration 10, loss = 2.09138439\n",
      "Validation score: 0.472420\n",
      "Iteration 11, loss = 2.02751103\n",
      "Validation score: 0.485178\n",
      "Iteration 12, loss = 1.96381608\n",
      "Validation score: 0.504315\n",
      "Iteration 13, loss = 1.90133301\n",
      "Validation score: 0.518199\n",
      "Iteration 14, loss = 1.84070515\n",
      "Validation score: 0.526829\n",
      "Iteration 15, loss = 1.78233183\n",
      "Validation score: 0.539212\n",
      "Iteration 16, loss = 1.72692004\n",
      "Validation score: 0.554221\n",
      "Iteration 17, loss = 1.67347221\n",
      "Validation score: 0.569231\n",
      "Iteration 18, loss = 1.62359102\n",
      "Validation score: 0.580863\n",
      "Iteration 19, loss = 1.57642812\n",
      "Validation score: 0.596248\n",
      "Iteration 20, loss = 1.53165761\n",
      "Validation score: 0.601876\n",
      "Iteration 21, loss = 1.49018325\n",
      "Validation score: 0.609006\n",
      "Iteration 22, loss = 1.45160493\n",
      "Validation score: 0.615009\n",
      "Iteration 23, loss = 1.41498859\n",
      "Validation score: 0.621388\n",
      "Iteration 24, loss = 1.38042785\n",
      "Validation score: 0.629644\n",
      "Iteration 25, loss = 1.34781012\n",
      "Validation score: 0.634897\n",
      "Iteration 26, loss = 1.31672811\n",
      "Validation score: 0.641651\n",
      "Iteration 27, loss = 1.28737942\n",
      "Validation score: 0.645779\n",
      "Iteration 28, loss = 1.26014236\n",
      "Validation score: 0.656660\n",
      "Iteration 29, loss = 1.23432726\n",
      "Validation score: 0.657786\n",
      "Iteration 30, loss = 1.20872824\n",
      "Validation score: 0.663790\n",
      "Iteration 31, loss = 1.18596384\n",
      "Validation score: 0.666792\n",
      "Iteration 32, loss = 1.16304826\n",
      "Validation score: 0.671670\n",
      "Iteration 33, loss = 1.14110347\n",
      "Validation score: 0.678799\n",
      "Iteration 34, loss = 1.12064891\n",
      "Validation score: 0.680675\n",
      "Iteration 35, loss = 1.10109604\n",
      "Validation score: 0.687430\n",
      "Iteration 36, loss = 1.08274286\n",
      "Validation score: 0.693433\n",
      "Iteration 37, loss = 1.06439979\n",
      "Validation score: 0.696435\n",
      "Iteration 38, loss = 1.04706983\n",
      "Validation score: 0.697561\n",
      "Iteration 39, loss = 1.03115522\n",
      "Validation score: 0.700563\n",
      "Iteration 40, loss = 1.01610675\n",
      "Validation score: 0.704315\n",
      "Iteration 41, loss = 1.00025728\n",
      "Validation score: 0.712195\n",
      "Iteration 42, loss = 0.98622591\n",
      "Validation score: 0.712570\n",
      "Iteration 43, loss = 0.97303974\n",
      "Validation score: 0.711069\n",
      "Iteration 44, loss = 0.95998683\n",
      "Validation score: 0.715197\n",
      "Iteration 45, loss = 0.94669002\n",
      "Validation score: 0.721201\n",
      "Iteration 46, loss = 0.93485932\n",
      "Validation score: 0.723077\n",
      "Iteration 47, loss = 0.92332851\n",
      "Validation score: 0.723452\n",
      "Iteration 48, loss = 0.91228734\n",
      "Validation score: 0.727205\n",
      "Iteration 49, loss = 0.90120778\n",
      "Validation score: 0.730582\n",
      "Iteration 50, loss = 0.89111134\n",
      "Validation score: 0.735084\n",
      "Iteration 51, loss = 0.88043713\n",
      "Validation score: 0.733208\n",
      "Iteration 52, loss = 0.87061072\n",
      "Validation score: 0.737711\n",
      "Iteration 53, loss = 0.86136146\n",
      "Validation score: 0.738462\n",
      "Iteration 54, loss = 0.85261637\n",
      "Validation score: 0.739212\n",
      "Iteration 55, loss = 0.84338485\n",
      "Validation score: 0.743340\n",
      "Iteration 56, loss = 0.83505997\n",
      "Validation score: 0.744841\n",
      "Iteration 57, loss = 0.82761382\n",
      "Validation score: 0.744841\n",
      "Iteration 58, loss = 0.81910086\n",
      "Validation score: 0.748218\n",
      "Iteration 59, loss = 0.81121355\n",
      "Validation score: 0.751595\n",
      "Iteration 60, loss = 0.80318621\n",
      "Validation score: 0.751595\n",
      "Iteration 61, loss = 0.79561492\n",
      "Validation score: 0.753096\n",
      "Iteration 62, loss = 0.78840038\n",
      "Validation score: 0.755722\n",
      "Iteration 63, loss = 0.78148307\n",
      "Validation score: 0.760225\n",
      "Iteration 64, loss = 0.77467751\n",
      "Validation score: 0.758349\n",
      "Iteration 65, loss = 0.76848200\n",
      "Validation score: 0.757974\n",
      "Iteration 66, loss = 0.76211462\n",
      "Validation score: 0.759850\n",
      "Iteration 67, loss = 0.75553926\n",
      "Validation score: 0.761726\n",
      "Iteration 68, loss = 0.74929579\n",
      "Validation score: 0.763977\n",
      "Iteration 69, loss = 0.74357442\n",
      "Validation score: 0.768105\n",
      "Iteration 70, loss = 0.73845514\n",
      "Validation score: 0.768105\n",
      "Iteration 71, loss = 0.73268758\n",
      "Validation score: 0.766604\n",
      "Iteration 72, loss = 0.72714456\n",
      "Validation score: 0.766604\n",
      "Iteration 73, loss = 0.72188923\n",
      "Validation score: 0.768856\n",
      "Iteration 74, loss = 0.71633211\n",
      "Validation score: 0.768856\n",
      "Iteration 75, loss = 0.71134479\n",
      "Validation score: 0.766979\n",
      "Iteration 76, loss = 0.70628380\n",
      "Validation score: 0.767730\n",
      "Iteration 77, loss = 0.70114957\n",
      "Validation score: 0.768480\n",
      "Iteration 78, loss = 0.69665412\n",
      "Validation score: 0.770732\n",
      "Iteration 79, loss = 0.69236085\n",
      "Validation score: 0.771482\n",
      "Iteration 80, loss = 0.68730557\n",
      "Validation score: 0.770732\n",
      "Iteration 81, loss = 0.68327515\n",
      "Validation score: 0.768480\n",
      "Iteration 82, loss = 0.67893095\n",
      "Validation score: 0.771857\n",
      "Iteration 83, loss = 0.67454373\n",
      "Validation score: 0.772233\n",
      "Iteration 84, loss = 0.67055995\n",
      "Validation score: 0.773358\n",
      "Iteration 85, loss = 0.66645393\n",
      "Validation score: 0.771857\n",
      "Iteration 86, loss = 0.66197778\n",
      "Validation score: 0.774109\n",
      "Iteration 87, loss = 0.65918231\n",
      "Validation score: 0.773734\n",
      "Iteration 88, loss = 0.65485655\n",
      "Validation score: 0.773734\n",
      "Iteration 89, loss = 0.65133371\n",
      "Validation score: 0.775610\n",
      "Iteration 90, loss = 0.64743021\n",
      "Validation score: 0.774859\n",
      "Iteration 91, loss = 0.64405468\n",
      "Validation score: 0.774859\n",
      "Iteration 92, loss = 0.64035555\n",
      "Validation score: 0.771857\n",
      "Iteration 93, loss = 0.63684342\n",
      "Validation score: 0.774109\n",
      "Iteration 94, loss = 0.63319855\n",
      "Validation score: 0.776735\n",
      "Iteration 95, loss = 0.62956673\n",
      "Validation score: 0.774484\n",
      "Iteration 96, loss = 0.62672603\n",
      "Validation score: 0.776360\n",
      "Iteration 97, loss = 0.62345100\n",
      "Validation score: 0.777486\n",
      "Iteration 98, loss = 0.62067889\n",
      "Validation score: 0.780488\n",
      "Iteration 99, loss = 0.61746960\n",
      "Validation score: 0.774859\n",
      "Iteration 100, loss = 0.61481170\n",
      "Validation score: 0.775610\n",
      "Iteration 101, loss = 0.61194131\n",
      "Validation score: 0.777486\n",
      "Iteration 102, loss = 0.60871813\n",
      "Validation score: 0.779362\n",
      "Iteration 103, loss = 0.60588595\n",
      "Validation score: 0.777111\n",
      "Iteration 104, loss = 0.60280148\n",
      "Validation score: 0.780113\n",
      "Iteration 105, loss = 0.59983591\n",
      "Validation score: 0.780863\n",
      "Iteration 106, loss = 0.59754984\n",
      "Validation score: 0.779362\n",
      "Iteration 107, loss = 0.59522120\n",
      "Validation score: 0.780113\n",
      "Iteration 108, loss = 0.59198519\n",
      "Validation score: 0.778987\n",
      "Iteration 109, loss = 0.58950249\n",
      "Validation score: 0.779362\n",
      "Iteration 110, loss = 0.58679962\n",
      "Validation score: 0.781989\n",
      "Iteration 111, loss = 0.58446590\n",
      "Validation score: 0.783865\n",
      "Iteration 112, loss = 0.58193824\n",
      "Validation score: 0.783490\n",
      "Iteration 113, loss = 0.57982173\n",
      "Validation score: 0.782739\n",
      "Iteration 114, loss = 0.57817542\n",
      "Validation score: 0.783114\n",
      "Iteration 115, loss = 0.57646972\n",
      "Validation score: 0.779737\n",
      "Iteration 116, loss = 0.57421980\n",
      "Validation score: 0.781614\n",
      "Iteration 117, loss = 0.57162597\n",
      "Validation score: 0.784240\n",
      "Iteration 118, loss = 0.56921660\n",
      "Validation score: 0.781989\n",
      "Iteration 119, loss = 0.56695200\n",
      "Validation score: 0.784991\n",
      "Iteration 120, loss = 0.56406462\n",
      "Validation score: 0.784240\n",
      "Iteration 121, loss = 0.56133985\n",
      "Validation score: 0.785741\n",
      "Iteration 122, loss = 0.55971904\n",
      "Validation score: 0.786116\n",
      "Iteration 123, loss = 0.55761184\n",
      "Validation score: 0.787617\n",
      "Iteration 124, loss = 0.55513684\n",
      "Validation score: 0.787242\n",
      "Iteration 125, loss = 0.55320095\n",
      "Validation score: 0.784615\n",
      "Iteration 126, loss = 0.55193990\n",
      "Validation score: 0.786492\n",
      "Iteration 127, loss = 0.54915784\n",
      "Validation score: 0.783865\n",
      "Iteration 128, loss = 0.54803153\n",
      "Validation score: 0.787617\n",
      "Iteration 129, loss = 0.54640312\n",
      "Validation score: 0.787242\n",
      "Iteration 130, loss = 0.54374773\n",
      "Validation score: 0.784615\n",
      "Iteration 131, loss = 0.54183162\n",
      "Validation score: 0.786116\n",
      "Iteration 132, loss = 0.53984105\n",
      "Validation score: 0.786867\n",
      "Iteration 133, loss = 0.53791550\n",
      "Validation score: 0.784991\n",
      "Iteration 134, loss = 0.53563473\n",
      "Validation score: 0.785366\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 0.7837879018741429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       greek       0.77      0.48      0.59       164\n",
      " southern_us       0.61      0.46      0.52       271\n",
      "    filipino       0.76      0.66      0.71       530\n",
      "      indian       0.79      0.84      0.81       835\n",
      "    jamaican       0.70      0.55      0.61       225\n",
      "     spanish       0.62      0.65      0.64       911\n",
      "     italian       0.77      0.70      0.74       382\n",
      "     mexican       0.87      0.90      0.89       992\n",
      "     chinese       0.68      0.46      0.55       222\n",
      "     british       0.81      0.89      0.85      2581\n",
      "        thai       0.83      0.65      0.73       150\n",
      "  vietnamese       0.81      0.72      0.76       488\n",
      "cajun_creole       0.79      0.74      0.77       275\n",
      "   brazilian       0.90      0.93      0.91      2138\n",
      "      french       0.82      0.76      0.79       261\n",
      "    japanese       0.61      0.45      0.52       155\n",
      "       irish       0.70      0.80      0.74      1426\n",
      "      korean       0.62      0.49      0.55       320\n",
      "    moroccan       0.79      0.77      0.78       532\n",
      "     russian       0.66      0.54      0.60       268\n",
      "\n",
      "    accuracy                           0.78     13126\n",
      "   macro avg       0.75      0.67      0.70     13126\n",
      "weighted avg       0.78      0.78      0.78     13126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpc = MLPClassifier(random_state=1, \n",
    "    # max_iter=150, #max_iter =300\n",
    "    early_stopping=True,\n",
    "    batch_size=int(np.round(X.shape[0]/10)),\n",
    "    verbose=True,    \n",
    "    activation='logistic',\n",
    "    learning_rate='adaptive'\n",
    "    ).fit(X_train, y_train)\n",
    "y_pred = mlpc.predict(X_test)\n",
    "print('Accuracy: '+str(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=train.cuisine.unique()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7837879018741429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       greek       0.77      0.48      0.59       164\n",
      " southern_us       0.61      0.46      0.52       271\n",
      "    filipino       0.76      0.66      0.71       530\n",
      "      indian       0.79      0.84      0.81       835\n",
      "    jamaican       0.70      0.55      0.61       225\n",
      "     spanish       0.62      0.65      0.64       911\n",
      "     italian       0.77      0.70      0.74       382\n",
      "     mexican       0.87      0.90      0.89       992\n",
      "     chinese       0.68      0.46      0.55       222\n",
      "     british       0.81      0.89      0.85      2581\n",
      "        thai       0.83      0.65      0.73       150\n",
      "  vietnamese       0.81      0.72      0.76       488\n",
      "cajun_creole       0.79      0.74      0.77       275\n",
      "   brazilian       0.90      0.93      0.91      2138\n",
      "      french       0.82      0.76      0.79       261\n",
      "    japanese       0.61      0.45      0.52       155\n",
      "       irish       0.70      0.80      0.74      1426\n",
      "      korean       0.62      0.49      0.55       320\n",
      "    moroccan       0.79      0.77      0.78       532\n",
      "     russian       0.66      0.54      0.60       268\n",
      "\n",
      "    accuracy                           0.78     13126\n",
      "   macro avg       0.75      0.67      0.70     13126\n",
      "weighted avg       0.78      0.78      0.78     13126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlpc.predict(X_test)\n",
    "print('Accuracy: '+str(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=train.cuisine.unique()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Imports\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (3600,)\n",
      "Epoch 1/30\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 2.8186 - accuracy: 0.1733 - val_loss: 2.5151 - val_accuracy: 0.3310\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 2.3794 - accuracy: 0.3715 - val_loss: 2.1738 - val_accuracy: 0.4617\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 2.0383 - accuracy: 0.5017 - val_loss: 1.8731 - val_accuracy: 0.5328\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 1.7523 - accuracy: 0.5551 - val_loss: 1.6420 - val_accuracy: 0.5747\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.5209 - accuracy: 0.6046 - val_loss: 1.4459 - val_accuracy: 0.6165\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 1.3294 - accuracy: 0.6479 - val_loss: 1.2886 - val_accuracy: 0.6467\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 1.1760 - accuracy: 0.6815 - val_loss: 1.1647 - val_accuracy: 0.6775\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 1.0535 - accuracy: 0.7192 - val_loss: 1.0653 - val_accuracy: 0.7071\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.9510 - accuracy: 0.7439 - val_loss: 0.9882 - val_accuracy: 0.7250\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.8707 - accuracy: 0.7654 - val_loss: 0.9309 - val_accuracy: 0.7370\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.8073 - accuracy: 0.7796 - val_loss: 0.8875 - val_accuracy: 0.7452\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 0.7557 - accuracy: 0.7920 - val_loss: 0.8545 - val_accuracy: 0.7557\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.7130 - accuracy: 0.8033 - val_loss: 0.8308 - val_accuracy: 0.7546\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 0s 88ms/step - loss: 0.6772 - accuracy: 0.8087 - val_loss: 0.8123 - val_accuracy: 0.7587\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6455 - accuracy: 0.8176 - val_loss: 0.7934 - val_accuracy: 0.7664\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6163 - accuracy: 0.8249 - val_loss: 0.7825 - val_accuracy: 0.7705\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5930 - accuracy: 0.8306 - val_loss: 0.7746 - val_accuracy: 0.7720\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.5705 - accuracy: 0.8369 - val_loss: 0.7671 - val_accuracy: 0.7747\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.5502 - accuracy: 0.8426 - val_loss: 0.7599 - val_accuracy: 0.7741\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5320 - accuracy: 0.8476 - val_loss: 0.7575 - val_accuracy: 0.7724\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5155 - accuracy: 0.8505 - val_loss: 0.7591 - val_accuracy: 0.7726\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.5014 - accuracy: 0.8547 - val_loss: 0.7541 - val_accuracy: 0.7735\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.4859 - accuracy: 0.8595 - val_loss: 0.7511 - val_accuracy: 0.7777\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.4700 - accuracy: 0.8657 - val_loss: 0.7475 - val_accuracy: 0.7805\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.4556 - accuracy: 0.8687 - val_loss: 0.7487 - val_accuracy: 0.7784\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.4444 - accuracy: 0.8710 - val_loss: 0.7537 - val_accuracy: 0.7765\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.4325 - accuracy: 0.8744 - val_loss: 0.7531 - val_accuracy: 0.7803\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.4214 - accuracy: 0.8763 - val_loss: 0.7535 - val_accuracy: 0.7801\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.4069 - accuracy: 0.8830 - val_loss: 0.7582 - val_accuracy: 0.7799\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.3977 - accuracy: 0.8863 - val_loss: 0.7590 - val_accuracy: 0.7812\n",
      "411/411 [==============================] - 3s 6ms/step - loss: 0.7641 - accuracy: 0.7746\n",
      "Test results - Loss: 0.7641235589981079 - Accuracy: 0.774569571018219%\n"
     ]
    }
   ],
   "source": [
    "# Configuration options\n",
    "feature_vector_length = X.shape[1]\n",
    "num_classes = pd.unique(y).shape[0]\n",
    "\n",
    "\n",
    "# Reshape the data - MLPs do not understand such things as '2D'.\n",
    "# Reshape to 28 x 28 pixels = 784 features\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_vector_length)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_vector_length)\n",
    "\n",
    "# Convert into greyscale\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "# Convert target classes to categorical ones\n",
    "Y_train = to_categorical(y_train, num_classes)\n",
    "Y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Set the input shape\n",
    "input_shape = (feature_vector_length,)\n",
    "print(f'Feature shape: {input_shape}')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(350, input_shape=input_shape, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Configure the model and start training\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(X_train, Y_train, epochs=10, batch_size=250, verbose=1, validation_split=0.2)\n",
    "model.fit(X_train, Y_train, epochs=10*3, batch_size=int(np.round(X.shape[0]/10)), verbose=1, validation_split=0.2)\n",
    "\n",
    "# Test the model after training\n",
    "test_results = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# # svc = make_pipeline(StandardScaler(), SVC(gamma='auto', verbose = True, cache_size=7000))\n",
    "# # svc = SVC(gamma='auto', verbose = True,cache_size=int(np.round(X.shape[0]/10)))\n",
    "# svc = SVC(gamma='auto', verbose = True,cache_size=10)\n",
    "# svc.fit(X_train, y_train)\n",
    "# y_pred = svc.predict(X_test)\n",
    "# print('Accuracy: '+str(accuracy_score(y_test, y_pred)))\n",
    "# print(classification_report(y_test, y_pred, target_names=train.cuisine.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization (LGBM, Transformer, MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dd76241fa2ee905525e0a1efacb16294be0e14843e7f10299cba718b3e5fb2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
